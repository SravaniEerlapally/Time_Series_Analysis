1.What is Time Series Data?
A) Observations collected over a time intervals i.e monthly, weekly, daily, seconds.
     Eg: Stock data, sales data, weather data, social media activity
 CHARACTERISTICS:
 ==> Chronological Order (regular intervals)
 ==> Sequential Order (cuurent obs depends on past )
 ==> Temporal Components ( trend, seasonality, cycle, noise)
 ==> Constant Frequency (continuous data without any missing value)
 ==> Dynamic Nature (affected by external factors)
 BEFORE PERFORMING TSA, these all should satisfy if not we need to change.

2.TIME SERIES ANALAYSIS
   - Its a statistical technique i.e used to extract meaningful insights about patterns and trends.
 Steps in TSA:
  i) Understand the past data
 ii) Forecating Future
Charts we prefer are Line Chart and Candle stick Chart[For Stocks(in 1 chart high,low, open,close)]
To compare with other data with same domain , unit we can plot in same and compare with multiple lines

3.TIME SERIES DECOMPOSITION
   ->Trend: Long Term Direction of Data [/Upward][\Downward]
   ->Seasonaity: Repeating patterns(similar) in fixed intervals
   ->Cyclic(not fixed):Cyclic is a fluctuation that occurs in data that is not related to seasonality.
                      Cycles are usually longer than seasonal and difficult to predict because they are irregular.
                      Cyclicity can be caused by economic, political, or social factors that influence the data.
   ->Residual (Noise): Sudden or random fluctuation

4.Types Of Decomposition
         --->1.Additive yt = Tt+St+Rt
                when the seasonal variation is relatively constant over time
                Parallel, seasonilty decreases trend increases

         --->2.Multiplicative yt = Tt*St*Rt
                when the seasonal variation increases over time.
                proportional, seasonaility increases trend increases

 STL(Seasonal & Trend) Decomposition using LOESS(Locally Estimated Scatterplot Smoothening)
   - best method to handle outliers in time series data
   key diff
   Classical Decomposition                                                           STL Decompositon
  - fixed seasonal pattern                                     - variety seasonal pattern
  - easily influenced by outliers                              - handles outliers
  - handle both additive & multiplicative timeseries data      - only handle additive timeseries data

:: When TSD is based on additive model we use STL Decomposition , or if its based multiplicative we use CLASSICAL Decomposition

5.Stationarity
  If Mean, Variance, Auto-Correlation of any time series data are constant.
  Constant Mean i.e No trend

Q) Why do we need stationarity in TSD>
A) Helps in future forecasting easier because it assumes same statistical properties through the time.
   Forecasting models requires a stationary data
  Types Of Stationarity
     i) Weak Stationarity
          Constant mean, variance, auto-correlation.
          Joint Distribution can change.
        Use Case: For different Forecating model picking shorter period of data
    ii) Strict Stationarity
           Exhibits the properties of Weak Stationarity.
           Joint Distribution remains unchanged when shifted along any time period. 
          Use Case: Modelling entire distribution of data such that forecasting will be closure to actual values.

6.Testing For Stationarity
  i)Weak
     -ADF Test
        If Timeseries data has unit root 
        [unit root means non-stationary trend ]
     H0: data has unit root(non-stationary trend)
     H1: data not has unit root(stationary trend)
     Decision : If p<0.05 ->stationary (strong evidence against null hyp) we accept Alt hypothesis.
                If ADF Statistic<critical value, we reject the null hypothesis.

     -KPSS Test
        Fits a constant mean model on the data
        It measures the variation of the cumulative sum of residual
      H0: stationary trend
      H1: non-stationary trend
     Decision : If p<0.05 ->non-stationary (strong evidence against null hyp) we accept Alt hypothesis.
                If KPSS Statistic>critical value, we reject the null hypothesis.
   After these 2tests you get then it is stationary.

  ii)Strict 
    to test 1st we check week or not bcoz it must follow week statioary criteria, once done we'll check other test
      -KS Test(Kolmogorov Simrnov Test)
        Compare the cumulative distribution functions of 2samples
       Decision : If p>0.05, no diff in distribution 

7.Making TimeSeries Data From Non-Stationary to Stationary
 There are 4 methods:
 i)Differencing:
    yt = current value
    yt-1 = past observed value
    1st order Difference yt`= yt-yt-1
    2nd order Difference yt``= yt`-yt`-1
ii)Transformation
     -it stabilizes the variance of time series data
   1. Logarithmic - log of data
   2. Power - sqroot yt/ sqroot yt-1
   3. Box-Cox(always +ve) - Combination of Power & Logarthmic
iii)Detrending
      -removing trend component
  1. Linear Detrending
  2. Moving Average Detrending (window size moving)
  3. Seasonality Adjustment : removing seasonal component
        STL decomposition

8.WHITE NOISE & RANDOM WALK
 1) WHITE NOISE: no pattern, trend, or seasonality
    properties:
     ->same mean, const variance , no auto-correlation
Whenever you get white noise timesseries data i.e completely random and we should not move forward , we can't prdeict.

 2) Random Walk: cumulative pattern
     yt = yt-1 + et (where et=white noise)
     it is predictable
     eg: stock price
    Properties:
     -> means & variance change overtime
     -> Random walk is a completely non-stationary data ,1st diff is stationary
         we predict only after differentiation before that it is non-predictable as it is non-stationary data
     -> no predictable pattern

9. Identifying WHITE NOISE & RANDOM WALK
   i)By Visually(checking the mean, variance , auto-correlation in plots)
   ii)ACF & PACF Plot(choose the order of the model & predict white noise/random)
   iii)Ljung Box Test(used to check auto-correlation)
        if p<0.05 random walk or else white noise
        H0: no auto-correlation, H1: auto-correlation




  












    
